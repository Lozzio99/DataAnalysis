{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Analysis \n",
    "# Linear Regression 2\n",
    "\n",
    "In this lecture (a continuation of the previous one), we'll discuss:\n",
    "4. overfitting, model generalizability, the bias-variance tradeoff and cross-validation\n",
    "5. regularization\n",
    "\n",
    "Recommended reading:\n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 5.1, 6.1, 6.2 [digital version available here](https://www.statlearning.com/)\n",
    "\n",
    "* Sebastian Raschka: Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning (https://arxiv.org/abs/1811.12808)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn import linear_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#This is a helper function to assist with creating polynomial features\n",
    "\n",
    "def make_features(train_set, test_set, degrees):\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    for d in degrees:\n",
    "        traintestdict={}\n",
    "        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
    "        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Cross-validation\n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a predictive model (regression, classification,...) will *generalize* to an independent data set. In regression, cross-validation is a method for assessing how well the regression model will predict the dependent value for points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "1. Split the dataset into two groups: the training dataset and the testing dataset. \n",
    "+ Train a variety of models on the training dataset. \n",
    "+ Check the accuracy of each model on the testing dataset. \n",
    "+ By comparing these accuracies, determine which model is best.\n",
    "\n",
    "Let's see this concept for the relationship between mpg and horsepower in the Auto dataset. We'll use the `scikit-learn` package for the cross validation analysis instead of `statsmodels`, because it is much easier to do cross validation there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n0  18.0          8         307.0         130    3504          12.0    70   \n1  15.0          8         350.0         165    3693          11.5    70   \n2  18.0          8         318.0         150    3436          11.0    70   \n3  16.0          8         304.0         150    3433          12.0    70   \n4  17.0          8         302.0         140    3449          10.5    70   \n\n   origin                       name  \n0       1  chevrolet chevelle malibu  \n1       1          buick skylark 320  \n2       1         plymouth satellite  \n3       1              amc rebel sst  \n4       1                ford torino  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mpg</th>\n      <th>cylinders</th>\n      <th>displacement</th>\n      <th>horsepower</th>\n      <th>weight</th>\n      <th>acceleration</th>\n      <th>year</th>\n      <th>origin</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>307.0</td>\n      <td>130</td>\n      <td>3504</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>chevrolet chevelle malibu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15.0</td>\n      <td>8</td>\n      <td>350.0</td>\n      <td>165</td>\n      <td>3693</td>\n      <td>11.5</td>\n      <td>70</td>\n      <td>1</td>\n      <td>buick skylark 320</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18.0</td>\n      <td>8</td>\n      <td>318.0</td>\n      <td>150</td>\n      <td>3436</td>\n      <td>11.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>plymouth satellite</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16.0</td>\n      <td>8</td>\n      <td>304.0</td>\n      <td>150</td>\n      <td>3433</td>\n      <td>12.0</td>\n      <td>70</td>\n      <td>1</td>\n      <td>amc rebel sst</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17.0</td>\n      <td>8</td>\n      <td>302.0</td>\n      <td>140</td>\n      <td>3449</td>\n      <td>10.5</td>\n      <td>70</td>\n      <td>1</td>\n      <td>ford torino</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = pd.read_csv('Auto.csv') #load data\n",
    "\n",
    "# one of the horsepowers is '?', so we just remove it and then map the remaining strings to integers\n",
    "auto = auto[auto.horsepower != '?']\n",
    "auto['horsepower'] = auto['horsepower'].map(int)\n",
    "\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m lr \u001B[38;5;241m=\u001B[39m \u001B[43mLinearRegression\u001B[49m() \u001B[38;5;66;03m# create a linear regression object\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# with scikit-learn, we have to extract values from the pandas dataframe\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m8\u001B[39m): \n",
      "\u001B[1;31mNameError\u001B[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression() # create a linear regression object\n",
    "\n",
    "# with scikit-learn, we have to extract values from the pandas dataframe\n",
    "for d in np.arange(2,8): \n",
    "    auto['h'+str(d)] = auto['horsepower']**d\n",
    "\n",
    "X = auto[['horsepower','h2','h3','h4','h5','h6','h7']].values.reshape(auto['horsepower'].shape[0],7)\n",
    "y = auto['mpg'].values.reshape(auto['mpg'].shape[0],1)\n",
    "\n",
    "plt.scatter(X[:,0], y,  color='black',label='data')\n",
    "\n",
    "# make data for plotting\n",
    "xs = np.linspace(20, 250, num=100)\n",
    "Xs = np.zeros([100,7])\n",
    "Xs[:,0] = xs\n",
    "for d in np.arange(1,7): \n",
    "    Xs[:,d] = xs**(d+1)\n",
    "    \n",
    "for d in np.arange(1,8):     \n",
    "    lr.fit(X=X[:,:d], y=y)\n",
    "    plt.plot(xs, lr.predict(X=Xs[:,:d]), linewidth=3, label = \"d = \" + str(d) )\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 Cross validation using scikit-learn \n",
    "\n",
    "- In scikit-learn, you can use the [*train_test_split*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split the dataset into a training dataset and a test dataset. This split is supposed to be random (pay attention to which method you are using and how it does the splitting). Obviously, we might get a bad random split (if we are unlucky). There are ways to control for that (e.g. through K-fold cross validation see below).\n",
    "+ The *score* function returns the coefficient of determination, $R^2$, of the prediction.\n",
    "\n",
    "In the following code, I've split the data in an unusual way - taking the test set to be 90% - to illustrate the point more clearly. Typically, we might make the training set to be 70-90% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "plt.scatter(X_train[:,0], y_train,  color='green',label='training data')\n",
    "plt.scatter(X_test[:,0], y_test,  color='black',label='test data')\n",
    "err_train=[]\n",
    "err_val=[]\n",
    "\n",
    "for d in np.arange(1,8):     \n",
    "    lr.fit(X=X_train[:,:d], y=y_train)\n",
    "    errval = mean_squared_error(y_test, lr.predict(X_test[:,:d]))\n",
    "    errtrain = mean_squared_error(y_train, lr.predict(X_train[:,:d]))\n",
    "    err_train.append(errtrain)\n",
    "    err_val.append(errval)\n",
    "    print('d=', d, ', train: ', lr.score(X_train[:,:d], y_train), ' test: ', lr.score(X_test[:,:d], y_test))\n",
    "#    print('d=', d, ', train: ', lr.score(X_train[:,:d], y_train), ' test: ', lr.score(X_test[:,:d], y_test))\n",
    "    plt.plot(xs, lr.predict(X=Xs[:,:d]), linewidth=3, label = \"d = \" + str(d) )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#plt.plot(np.arange(1,8), err_train, marker='o', label='training error')\n",
    "plt.plot(np.arange(1,8), err_val, marker='x', label='validation error')\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 K-Fold cross-validation in Python \n",
    "\n",
    "Let us now do 5-fold cross-validation on our  data set.\n",
    "\n",
    "- Similarly as before, we could use the [*train_test_split*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split the dataset accordingly. However, python offers more functions to do this.\n",
    "-  We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 5 ways into 5 folds, train on 4 folds, and calculate the validation error on the remaining one. We then average the errors over the five folds to get a cross-validation error for that $d$. \n",
    "\n",
    "We will use `KFold` from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = auto['horsepower'].values.reshape(auto['horsepower'].shape[0],1)\n",
    "y = auto['mpg'].values.reshape(auto['mpg'].shape[0],1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "n_folds=5\n",
    "\n",
    "kfold = KFold(n_folds)\n",
    "list(kfold.split(range(79)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "What is wrong with the above? Why we must execute KFold with the line below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_folds, shuffle=True)\n",
    "list(kfold.split(range(79)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "d=20\n",
    "degrees=range(d+1)\n",
    "train_errors = np.zeros((d+1,5))\n",
    "valid_errors = np.zeros((d+1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fold = 0\n",
    "for train, valid in KFold(n_folds, shuffle=True).split(range(79)): # split data into train/test groups, 5 times\n",
    "    train_dict, valid_dict = make_features(X_train[train], X_train[valid], degrees)\n",
    "    for d in degrees:\n",
    "        model = LinearRegression()\n",
    "        model.fit(train_dict[d], y_train[train]) # fit\n",
    "        train_errors[d, fold] = mean_squared_error(y_train[train], model.predict(train_dict[d])) # evaluate score function on held-out data\n",
    "        valid_errors[d, fold] = mean_squared_error(y_train[valid], model.predict(valid_dict[d])) # evaluate score function on held-out data\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We average the MSEs over the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_train_errors = train_errors.mean(axis=1)\n",
    "mean_valid_errors = valid_errors.mean(axis=1)\n",
    "std_train_errors = train_errors.std(axis=1)\n",
    "std_valid_errors = valid_errors.std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We find the degree that minimizes the `cross-validation` error, and just like before, refit the model on the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mindeg = np.argmin(mean_valid_errors)\n",
    "print(mindeg)\n",
    "post_cv_train_dict, test_dict=make_features(X_train, X_test, degrees)\n",
    "\n",
    "#fit on whole training set now.\n",
    "model = LinearRegression()\n",
    "model.fit(post_cv_train_dict[mindeg], y_train) # fit\n",
    "pred = model.predict(test_dict[mindeg])\n",
    "err = mean_squared_error(pred, y_test)\n",
    "errtr=mean_squared_error(y_train, model.predict(post_cv_train_dict[mindeg]))\n",
    "c0=sns.color_palette()[0]\n",
    "c1=sns.color_palette()[1]\n",
    "#plt.errorbar(degrees, [r[0] for r in results], yerr=[r[3] for r in results], marker='o', label='CV error', alpha=0.5)\n",
    "plt.plot(degrees, mean_train_errors, marker='o', label='Train error', alpha=0.9)\n",
    "plt.plot(degrees, mean_valid_errors, marker='o', label='CV error', alpha=0.9)\n",
    "\n",
    "\n",
    "plt.fill_between(degrees, mean_valid_errors-std_valid_errors, mean_valid_errors+std_valid_errors, color=c1, alpha=0.2)\n",
    "\n",
    "\n",
    "plt.plot([mindeg], [err], 'o',  label='test set error')\n",
    "\n",
    "plt.ylabel('mean squared error')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper right')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. Controling for overfitting: Regularization\n",
    "\n",
    "The subset selection methods use least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all predictors using a technique that constrains or regularizes the coefficient estimates, or or equivalently, that shrinks the  coefficient estimates towards zero. Think for a moment, why we want to squeeze them to zero?\n",
    "\n",
    "In the ordinary least squares problem we minimize the loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\mathbf{\\beta}^T \\mathbf{x}_i)^2,\n",
    "\\end{equation}\n",
    "\n",
    "to determine regression coefficients $\\mathbf{\\beta}$.  Here $y_i$ is the response variable for observation $i$, and $\\mathbf{x}_i$ is a vector from the predictor matrix  corresponding to observation $i$.\n",
    "\n",
    "\n",
    "The general idea behind regularization is to penalize the loss function to account for possibly very large values of the coefficients $\\mathbf \\beta$.  The  aforementioned optimization problem is then adjusted accordingly.  Instead of minimizing $L(\\mathbf{\\beta})$, we minimize the regularized loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{reg}}(\\mathbf{\\beta}) = L(\\mathbf{\\beta}) + \\lambda R(\\mathbf{\\beta}),\n",
    "\\end{equation}\n",
    "\n",
    "where $R(\\mathbf{\\beta})$ is a penalty function and $\\lambda$ is a scalar that weighs the relative importance of this penalty.  In this course we will explore two regularized regression models, Ridge ($L2$) and LASSO ($L1$). In ridge regression, the penalty function is the sum of the squares of the parameters, giving the regularized loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{Ridge}}(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\mathbf{\\beta}^T \\mathbf{x}_i)^2 + \\lambda \\sum_{j = 1}^d \\beta_j^2.\n",
    "\\end{equation}\n",
    "\n",
    "In LASSO regression the penalty function is the sum of the magnitudes of the parameters, leading to\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{LASSO}}(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\mathbf{\\beta}^T \\mathbf{x}_i)^2 + \\lambda \\sum_{j = 1}^d |\\beta_j|.\n",
    "\\end{equation}\n",
    "\n",
    "We will show how these optimization problems can be solved with `sklearn` to determine the model parameters $\\mathbf \\beta$.  We will also show how to choose $\\lambda$ appropriately via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Going back to the problem of selecting a polynomial degree, then we have focused on finding the polynomial with the right degree of complexity $d=*$ given the data that we have.\n",
    "\n",
    "When we **regularize** we smooth or restrict the choices of the kinds of (up to) 5th order polynomials that we allow in our fits. That is, if we want to fit with a 5th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions that we allow. Note that sklearn uses the notation $\\alpha$ instead of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.1 Regularization of our model with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex $\\cal{H}_{5}$ polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset $S_*$ of the set of 5th order polynomials indexed by $\\alpha_*$.\n",
    "\n",
    "Some notes:\n",
    "1. there is an error on the diagram, the 13 there should actually be a 5).\n",
    "2. in textbooks we commonly use $\\lambda$ as the symbol for regularization parameter but in scikit-learn that is $\\alpha$.\n",
    "\n",
    "![m:caption](complexity-error-reg.png)\n",
    "\n",
    "Lets see what some of the $\\alpha$s do. The diagram below trains on the entire training set, for given values of $\\alpha$, minimizing the penalty-term-added training error.\n",
    "\n",
    "**Note that here we are doing the note so good thing of exhausting the test set for demonstration purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "#from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters.csv', index_col=0).dropna()\n",
    "df.index.name = 'Player'\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "dummies.info()\n",
    "print(dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the columns for which we created dummy variables\n",
    "dfnew = df.drop(['League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "dfnew = pd.concat([dfnew, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "dfnew.info()\n",
    "\n",
    "dfnew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us define a helper function to help us do the train/test splits. I also have the option of whether I want a further split to validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(df, n_samples=df.shape[0], validation=False):\n",
    "    if validation:\n",
    "        sample = df.sample(n=n_samples)\n",
    "\n",
    "        msk = np.random.rand(len(sample)) < 0.8\n",
    "        non_test = sample[msk]\n",
    "        test = sample[~msk]\n",
    "        \n",
    "        msk = np.random.rand(len(non_test)) < 0.7\n",
    "        train = non_test[msk]\n",
    "        validation = non_test[~msk]\n",
    "        \n",
    "        return train, validation, test\n",
    "    \n",
    "    else:\n",
    "        sample = df.sample(n=n_samples)\n",
    "\n",
    "        msk = np.random.rand(len(sample)) < 0.8\n",
    "        train = sample[msk]\n",
    "        test = sample[~msk]\n",
    "        \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['AtBat',\n",
    " 'Hits',\n",
    " 'HmRun',\n",
    " 'Runs',\n",
    " 'RBI',\n",
    " 'Walks',\n",
    " 'Years',\n",
    " 'CAtBat',\n",
    " 'CHits',\n",
    " 'CHmRun',\n",
    " 'CRuns',\n",
    " 'CRBI',\n",
    " 'CWalks',\n",
    " 'PutOuts',\n",
    " 'Assists',\n",
    " 'Errors',\n",
    " 'League_N',\n",
    " 'Division_W',\n",
    " 'NewLeague_N']\n",
    "all_predictors = cols\n",
    "\n",
    "\n",
    "train, validation, test = train_test_split(dfnew, validation=True)\n",
    "\n",
    "y_train = train['Salary'].values\n",
    "y_val = validation['Salary'].values\n",
    "y_test = test['Salary'].values\n",
    "\n",
    "X_train = train[all_predictors].values\n",
    "X_val = validation[all_predictors].values\n",
    "X_test = test[all_predictors].values\n",
    "\n",
    "#for simplicity, let's keep this\n",
    "X_non_test = np.vstack((X_train, X_val))\n",
    "y_non_test = np.hstack((y_train, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For demonstrating the same results, let's use the following split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_non_test = pd.read_csv('Hitters_X_train.csv', index_col=0)\n",
    "y_non_test = pd.read_csv('Hitters_y_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('Hitters_X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('Hitters_y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ridge_regression = Ridge(alpha=100, fit_intercept=True)\n",
    "ridge_regression.fit(X_non_test, y_non_test)\n",
    "\n",
    "print('Ridge regression model:\\n {} + {}^T . x'.format(ridge_regression.intercept_, ridge_regression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(ridge_regression.coef_.flatten(), index=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train R^2: {}, test R^2: {}'.format(ridge_regression.score(X_non_test,y_non_test),\n",
    "                                           ridge_regression.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Why we need to standardize the predictors?\n",
    "\n",
    "The standard least squares coefficient estimates are scale equivariant: multiplying a predictor value $X_j$ by a constant $c$ simply leads to a scaling of the least squares coefficient estimates by a factor of $1/c$. In other words, regardless of how the $j$-th predictor is scaled, the product $X_j\\cdot \\beta_j$ will remain the same.\n",
    "\n",
    "In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by  a constant, due to the sum of squared coefficients term in  the penalty part of the ridge regression objective  function.\n",
    "\n",
    "Therefore, it is best to apply ridge regression after standardizing the predictors, using the formula\n",
    "\n",
    "$$\n",
    "\\tilde x_{ij} = \\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{ij}-\\bar x_j)^2}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ridge_std = Ridge(alpha=10, fit_intercept=True)\n",
    "ridge_std.fit(scale(X_non_test), y_non_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Ridge regression model:\\n {} + {}^T . x'.format(ridge_std.intercept_, ridge_std.coef_))\n",
    "pd.Series(ridge_std.coef_.flatten(), index=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train R^2: {}, test R^2: {}'.format(ridge_std.score(scale(X_non_test),y_non_test),\n",
    "                                           ridge_std.score(scale(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#not increasing the # of iterations, might give you a convergence warning\n",
    "lasso_regression = Lasso(alpha=1, fit_intercept=True, max_iter=10000)\n",
    "lasso_regression.fit(X_non_test, y_non_test)\n",
    "\n",
    "print('Lasso regression model:\\n {} + {}^T . x'.format(lasso_regression.intercept_, lasso_regression.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Train R^2: {}, test R^2: {}'.format(lasso_regression.score(X_non_test, y_non_test), \n",
    "                                           lasso_regression.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.2 The Effect of the Regularization Parameter\n",
    "\n",
    "Let's see in more detail what is the effect of regularization. We will vary the regularization parameter from $10^{-5}$ i.e. $0.00001$ till $10^9$ i.e. $1000000000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reg_params = np.hstack((10.**np.arange(-5, 0), 10**np.arange(0, 10) + 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_R_sq = []\n",
    "test_R_sq = []\n",
    "coefs = []\n",
    "\n",
    "for reg in reg_params:\n",
    "    ridge_regression1 = Ridge(alpha=reg, fit_intercept=True)\n",
    "    ridge_regression1.fit(scale(X_non_test), y_non_test)\n",
    "    coefs.append(ridge_regression1.coef_)\n",
    "\n",
    "    \n",
    "    train_R_sq.append(ridge_regression1.score(scale(X_non_test),y_non_test))\n",
    "    test_R_sq.append(ridge_regression1.score(scale(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.plot(reg_params, train_R_sq, color='blue', label='train')\n",
    "ax.plot(reg_params, test_R_sq, color='green', label='test')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Regularization Parameter vs Test R^2')\n",
    "ax.set_xlabel('Regularization strength')\n",
    "ax.set_ylabel('R^2')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = df.Salary\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "\n",
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "ridge = Ridge()\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha=a)\n",
    "    ridge.fit(scale(X), y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Ridge coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_R_sq = []\n",
    "test_R_sq = []\n",
    "\n",
    "for reg in reg_params:\n",
    "    lasso_regression = Lasso(alpha=reg, max_iter=100000, fit_intercept=True)\n",
    "    lasso_regression.fit(X_non_test, y_non_test)\n",
    "    \n",
    "    train_R_sq.append(lasso_regression.score(X_non_test, y_non_test))\n",
    "    test_R_sq.append(lasso_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.plot(reg_params, train_R_sq, color='blue', label='train')\n",
    "ax.plot(reg_params, test_R_sq, color='green', label='test')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Regularization Parameter vs Test R^2')\n",
    "ax.set_xlabel('Regularization strength')\n",
    "ax.set_ylabel('R^2')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas*2:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X), y)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.axis('tight')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('coefficients')\n",
    "plt.title('Lasso coefficients as a function of the regularization');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.3 Cross Validation: Selecting the Regularization Parameter (check it at home -- it will come for the clinic as well)\n",
    "\n",
    "How do I select the regularization parameter? Simply, with cross-validation. In this case, we also use the MSE (mean squared error) as a performance metric. This just puts together what we discussed before (regularization + cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_non_test = np.vstack((X_train, X_val))\n",
    "y_non_test = np.hstack((y_train, y_val))\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "x_val_scores = []\n",
    "mse_val = []\n",
    "\n",
    "for reg in reg_params:\n",
    "    ridge_regression = Ridge(alpha=reg, fit_intercept=True)\n",
    "    \n",
    "    validation_R_sqs = []\n",
    "    mses = []\n",
    "    for train_index, val_index in kf.split(X_non_test):\n",
    "        X_train, X_val = X_non_test[train_index], X_non_test[val_index]\n",
    "        y_train, y_val = y_non_test[train_index], y_non_test[val_index]\n",
    "\n",
    "        \n",
    "        ridge_regression.fit(scale(X_train), y_train)\n",
    "        validation_R_sqs.append(ridge_regression.score(scale(X_val), y_val))\n",
    "        pred = ridge_regression.predict(scale(X_val))\n",
    "        mses.append(mean_squared_error(y_val, pred))\n",
    "\n",
    "        \n",
    "    x_val_scores.append(np.mean(validation_R_sqs))\n",
    "    mse_val.append(np.mean(mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.plot(reg_params, mse_val, color='blue')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title('Regularization Strength vs Cross Validation Score')\n",
    "ax.set_xlabel('Strength of Regularization')\n",
    "ax.set_ylabel('Cross Validation Score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_alpha = reg_params[np.argmin(mse_val)]\n",
    "\n",
    "ridge_regression = Ridge(alpha=best_alpha, fit_intercept=True)\n",
    "ridge_regression.fit(scale(X_non_test), y_non_test)\n",
    "pred = ridge_regression.predict(scale(X_test))\n",
    "mse_final = mean_squared_error(y_test, pred)\n",
    "#test_R_sq = (ridge_regression.score(scale(X_test), y_test))\n",
    "\n",
    "print('best regularization param is:', best_alpha)\n",
    "print('the MSE (test set) for ridge regression with alpha = {} is: {}'.format(best_alpha, mse_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(ridge.coef_.flatten(), index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Alternative approaches: Use `RidgeCV` and `LassoCV`\n",
    "\n",
    "(check the documentation carefully before you apply them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\n",
    "ridgecv.fit(scale(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ridge.set_params(alpha=ridgecv.alpha_)\n",
    "ridge.fit(scale(X_train), y_train)\n",
    "mean_squared_error(y_test, ridge.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(ridge.coef_.flatten(), index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lassocv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lassocv.fit(scale(X_non_test), y_non_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(scale(X_train), y_train)\n",
    "mean_squared_error(y_test, lasso.predict(scale(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Appendix\n",
    "\n",
    "## Step-wise selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One straightforward way to think about how to pick among different predictors (variables) in the model would be to try to check *all* combinations between *all* predictors and how much they affect performance. That is called \"best subset selection\". For computational reasons, best subset selection cannot be applied to datasets with a large number of predictors (why?).\n",
    "\n",
    "Step-wise selection can then be a viable alternative. We have two options:\n",
    "\n",
    "* forward selection: we start with one predictor, find the best model with only one predictor (based on a performance metric), move to models with two predictors (by keeping the one predictor fixed) etc.\n",
    "\n",
    "* backward selection: opposite as above, we start with a model with all predictors and reduce them one by one.\n",
    "\n",
    "It is not guaranteed to find the best possible model out of all $2^p$ models containing subsets of all predictors (why?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters.csv', index_col=0).dropna()\n",
    "df.index.name = 'Player'\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "dummies.info()\n",
    "print(dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the columns for which we created dummy variables\n",
    "dfnew = df.drop(['League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "# Define the feature set X.\n",
    "dfnew = pd.concat([dfnew, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\n",
    "dfnew.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('Hitters_X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('Hitters_y_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('Hitters_X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('Hitters_y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['AtBat',\n",
    " 'Hits',\n",
    " 'HmRun',\n",
    " 'Runs',\n",
    " 'RBI',\n",
    " 'Walks',\n",
    " 'Years',\n",
    " 'CAtBat',\n",
    " 'CHits',\n",
    " 'CHmRun',\n",
    " 'CRuns',\n",
    " 'CRBI',\n",
    " 'CWalks',\n",
    " 'PutOuts',\n",
    " 'Assists',\n",
    " 'Errors',\n",
    " 'League_N',\n",
    " 'Division_W',\n",
    " 'NewLeague_N']\n",
    "\n",
    "train, validation, test = train_test_split(dfnew, validation=True)\n",
    "\n",
    "y_train = train['Salary'].values\n",
    "y_val = validation['Salary'].values\n",
    "y_test = test['Salary'].values\n",
    "\n",
    "regression_model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Forward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#all_predictors = credit.columns.tolist()\n",
    "all_predictors = cols\n",
    "\n",
    "predictors = [([], 0)]\n",
    "\n",
    "regression_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "R_sq_fwd = []\n",
    "\n",
    "for k in range(1, len(all_predictors)):\n",
    "    best_k_minus_1 = predictors[-1][0]\n",
    "\n",
    "    new_predictors = list(set(all_predictors) - set(best_k_minus_1))\n",
    "    validation_R_sqs = []\n",
    "\n",
    "    for predictor in new_predictors:\n",
    "\n",
    "        k_predictors = best_k_minus_1 + [predictor]\n",
    "        \n",
    "        X_train = train[k_predictors].values\n",
    "        X_val = validation[k_predictors].values\n",
    "        \n",
    "        if k == 1:\n",
    "            X_train = X_train.reshape((len(X_train), 1))\n",
    "            \n",
    "        regression_model.fit(X_train, y_train)\n",
    "        validation_R_sqs.append(regression_model.score(X_val, y_val))\n",
    "    \n",
    "    best_k = best_k_minus_1 + [new_predictors[np.argmax(validation_R_sqs)]]\n",
    "    R_sq_fwd.append(np.max(validation_R_sqs))\n",
    "    predictors.append((best_k, np.max(validation_R_sqs)))\n",
    "\n",
    "\n",
    "X_train = train[all_predictors].values\n",
    "X_val = validation[all_predictors].values  \n",
    "regression_model.fit(X_train, y_train)\n",
    "\n",
    "predictors.append((all_predictors, regression_model.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_predictor_set = sorted(predictors, key=lambda t: t[1])[-1]\n",
    "\n",
    "X_train = train[best_predictor_set[0]].values\n",
    "X_val = validation[best_predictor_set[0]].values  \n",
    "X_test = test[best_predictor_set[0]].values  \n",
    "\n",
    "regression_model.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))\n",
    "\n",
    "print('best predictor set: {}\\nvalidation R^2: {}\\ntest R^2: {}'.format(best_predictor_set[0], best_predictor_set[1], regression_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#a very ugly plot, needs improvement\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(range(1, len(predictors)-1 ), R_sq_fwd, color='blue', label='R^2')\n",
    "\n",
    "ax.set_title('Number of Predictors vs Model Fitness')\n",
    "ax.set_xlabel('Number of Predictors')\n",
    "ax.set_ylabel('Validation $R^2$')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Backwards Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar processes can be applied to the backward process, i.e. start with all predictors and remove one at each step. Let's see the same process, but introduce another metric here, AIC.\n",
    "\n",
    "The Akaike Information Criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. The AIC criterion is defined for a large class of models fit by maximum likelihood.\n",
    "\n",
    "$$AIC = -2 \\cdot logL +2 \\cdot d$$\n",
    "\n",
    "where $L$ is the maximized value of the likelihood function for the estimated model and $d$ is the total number of parameters (here is the number of predictors).\n",
    "\n",
    "In the case of  the linear model with Gaussian errors, maximum likelihood and least squares are the same thing. More specifically for the linear model: $ -2 \\cdot logL = \\frac{RSS}{\\sigma^2} $. Obviously, I need to choose the model with the smallest $AIC$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tools import add_constant\n",
    "import statsmodels.api as sm1\n",
    "\n",
    "def get_aic(X_train, y_train):\n",
    "    X_train = add_constant(X_train)\n",
    "    model = sm1.OLS(y_train, X_train).fit()\n",
    "    return model.aic\n",
    "\n",
    "X_train = train[all_predictors].values\n",
    "predictors = [(all_predictors, get_aic(X_train, y_train))]\n",
    "\n",
    "for k in range(len(all_predictors), 1, -1):\n",
    "    best_k_predictors = predictors[-1][0]\n",
    "    aics = []\n",
    "    \n",
    "    for predictor in best_k_predictors:\n",
    "        k_minus_1 = list(set(best_k_predictors) - set([predictor]))\n",
    "        X_train = train[k_minus_1].values\n",
    "\n",
    "        aics.append(get_aic(X_train, y_train))\n",
    "    \n",
    "    best_k_minus_1 = list(set(best_k_predictors) - set([best_k_predictors[np.argmin(aics)]]))\n",
    "    predictors.append((best_k_minus_1, np.min(aics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_predictor_set = sorted(predictors, key=lambda t: t[1])[0]\n",
    "\n",
    "X_train = train[best_predictor_set[0]].values\n",
    "X_val = validation[best_predictor_set[0]].values  \n",
    "X_test = test[best_predictor_set[0]].values  \n",
    "regression_model.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))\n",
    "\n",
    "print('best predictor set: {}\\nAIC: {}\\ntest R^2: {}'.format(best_predictor_set[0], best_predictor_set[1], regression_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6. Extensions of the linear model\n",
    "\n",
    "The linear model can be generalized into many different variations:\n",
    "\n",
    "* Classification problems: Logistic regression, etc.\n",
    "\n",
    "* Non-linearity: splines, generalized additive models, etc.\n",
    "\n",
    "* Interactions: Tree-based methods (bagging, boosting, etc.)\n",
    "\n",
    "* Regularized fitting: We covered this already!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}